{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"learned_emb_model_wo_attn.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"90ql2AViQpEZ"},"source":["# French to English Machine Translation  \n","## Seq2Seq with learned embeddings "]},{"cell_type":"markdown","metadata":{"id":"b-zBMDqH1Dh0"},"source":["Import Libraries"]},{"cell_type":"code","metadata":{"id":"TWmY82DcY1Wn"},"source":["!pip install bcolz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzRfkzlyzGUN"},"source":["!pip install translate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGS7GnX8cK7G"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import os, pickle, collections, bcolz\n","import operator\n","import itertools \n","import gensim\n","from termcolor import colored\n","from translate import Translator\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTxV64RKESs1"},"source":["ANSI Color Codes for colorful text output"]},{"cell_type":"code","metadata":{"id":"KCHdcO6DEUtP"},"source":["bold_blue_font_tag = '\\x1b[1m\\x1b[34m'\n","bold_red_font_tag = '\\x1b[1m\\x1b[31m'\n","red_font_tag = '\\u001b[31m'\n","bold_green_font_tag = '\\x1b[1m\\x1b[32m'\n","magenta = '\\033[35m'\n","bold = '\\033[1m'\n","reset = '\\033[0m'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxQGUM7O1FQe"},"source":["Mount Drive"]},{"cell_type":"code","metadata":{"id":"2nQLXyxJgV4K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606773707325,"user_tz":300,"elapsed":187105,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"b07ced0e-22df-473f-ea3c-4e21dbc7d245"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pi92GVRXdSps"},"source":["Set the path to these files. They should be in the shared folder. Copy to your drive, or upload to colab instance."]},{"cell_type":"code","metadata":{"id":"H21QoI2MCjxr"},"source":["# dataset\n","dataset_path = '/content/drive/My Drive/MT/eng-fra.txt'\n","\n","# model weights\n","encoder_weights_path = '/content/drive/My Drive/encoder1_250000_0.0955.pth'\n","decoder_weights_path = '/content/drive/My Drive/attn_decoder1_250000_0.0955.pth'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ec-UKQQN1GsF"},"source":["Make sure GPU is available"]},{"cell_type":"code","metadata":{"id":"gaXqqV51_gFf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606773716671,"user_tz":300,"elapsed":1432,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"e6050934-52db-4eba-d742-013c80823b4a"},"source":["torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"Dbs8gFaI1JJi"},"source":["Create class to create vocab dictionaries"]},{"cell_type":"code","metadata":{"id":"qWoJwmC1cK7J"},"source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6d7mLUb1a3p"},"source":["Preprocess text: lowercase, remove some punc., convert to Ascii"]},{"cell_type":"code","metadata":{"id":"88Nzd4f1cK7M"},"source":["# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sM_er7DX1lAN"},"source":["Function to read sentences, preprocess text, create vocab dictionaries"]},{"cell_type":"code","metadata":{"id":"113KR-wYcK7P"},"source":["def readLangs(reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open(dataset_path, encoding='utf-8').read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang('fra')\n","        output_lang = Lang('eng')\n","    else:\n","        input_lang = Lang('eng')\n","        output_lang = Lang('fra')\n","\n","    return input_lang, output_lang, pairs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YI3n_WTG136n"},"source":["Filtering dataset by length and prefix"]},{"cell_type":"code","metadata":{"id":"LSXmvzAzcK7R"},"source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH and \\\n","        p[1].startswith(eng_prefixes)\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TN-C35pt17iz"},"source":["Calling everything above to prepare data"]},{"cell_type":"code","metadata":{"id":"o5UZvCZacK7W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606786651861,"user_tz":300,"elapsed":5057,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"7c42f20f-7ad0-4a06-f905-826d3ca1d82b"},"source":["def prepareData(reverse=False):\n","    input_lang, output_lang, pairs = readLangs(reverse)\n","    pre_len = len(pairs)\n","    print(\"\\nRead %s sentence pairs\" % pre_len)\n","    pairs = filterPairs(pairs)\n","    post_len = len(pairs)\n","    print(f'Trimmed to {post_len} sentence pairs ')\n","    print(f'Using {post_len/pre_len * 100:.2f}% of dataset')\n","\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData(True)\n","print(f'\\nExample pair of sentences: {random.choice(pairs)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading lines...\n","\n","Read 135842 sentence pairs\n","Trimmed to 10599 sentence pairs \n","Using 7.80% of dataset\n","\n","Example pair of sentences: ['ils bronzent autour de la piscine .', 'they re sunbathing around the pool .']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GZgm0vbr3Fdw"},"source":["Class for encoder"]},{"cell_type":"code","metadata":{"id":"LvFjh46ncK7Z"},"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xabdarwSHEWd"},"source":["Decoder without Attention"]},{"cell_type":"code","metadata":{"id":"F5l_zDxrHDJ_"},"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8YF2gYF3ggV"},"source":["Converting to tensors"]},{"cell_type":"code","metadata":{"id":"cMHtL8y5cK7h"},"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S9kiLobB3k7_"},"source":["Training function"]},{"cell_type":"code","metadata":{"id":"SzlcdNlScK7k"},"source":["teacher_forcing_ratio = 0.5\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khj-xZjx3pd0"},"source":["Functions to keep track of time"]},{"cell_type":"code","metadata":{"id":"lkW8qaLRcK7n"},"source":["import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rgt73wZ13sdd"},"source":["Function to run training"]},{"cell_type":"code","metadata":{"id":"0o956_VAcK7p"},"source":["def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [tensorsFromPair(random.choice(pairs))\n","                      for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = train(input_tensor, target_tensor, encoder,\n","                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)\n","    return plot_losses"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwU4mb0Q30bp"},"source":["Plot loss curve after training"]},{"cell_type":"code","metadata":{"id":"XIuDWzv6cK7s"},"source":["%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7RVKkrR34uX"},"source":["Function to evaluate after training"]},{"cell_type":"code","metadata":{"id":"jhEqrzfncK7u"},"source":["def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                     encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden)\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGqVSHVs380E"},"source":["Calculating bleu score"]},{"cell_type":"code","metadata":{"id":"OZOD5-adZYTO"},"source":["from nltk.translate.bleu_score import sentence_bleu\n","\n","def bleu(reference,candidate):\n","  one_gram = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n","  return (one_gram)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uw6IUwJzYHB_"},"source":["Calculating Gleu score"]},{"cell_type":"code","metadata":{"id":"XWGiS2VNV-1Z"},"source":["from nltk.translate.gleu_score import sentence_gleu\n","\n","def gleu(reference, candidate):\n","  one_gram = sentence_gleu(reference, candidate)\n","  return (one_gram)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ReBUvyIC8yNY"},"source":["Custom score"]},{"cell_type":"code","metadata":{"id":"9v1KTaagWMP1"},"source":["# create a custom score by averaging bs and gs scores with weights\n","# adding a bonus if different words have a shared semantic meaning (cs score > 0.3)\n","# subtracting points if predicted sequence has duplicated words\n","# inputs: bleu score, gleu score, # of double words in pred\n","# outputs: custom score\n","\n","def custom_score(bs,gs,cs,double_word_penalty, verbose=True):\n","  total = ((bs*.75)+(gs*.25)) # weighted avg\n","  cs_bonus = 0\n","\n","  # calc cs bonus\n","  def get_bonus(cs,multiplier = .4):\n","    additional = 0\n","    for i in cs:\n","      additional += i * multiplier\n","    return additional\n","\n","  # if we have similarities, compute bonus\n","  if cs: \n","    cs_bonus = get_bonus(cs)\n","\n","\n","  # if perfect score, return 1\n","  if bs == 1:\n","    if verbose:\n","      print('\\nSemantic similarity bonus : +', float(cs_bonus))\n","      print('Double word penalty:        -', double_word_penalty * .1,'\\n')\n","    return 1.00\n","\n","  else:\n","    if cs_bonus:\n","      grand_total = total + cs_bonus\n","      if grand_total < 1:\n","        if verbose:\n","          print('\\nSemantic similarity bonus : +', float(cs_bonus))\n","          print('Double word penalty:        -', double_word_penalty * .1,'\\n')\n","        return grand_total\n","\n","      # bonus put score over 1  \n","      else:\n","        cs_bonus = get_bonus(cs, multiplier = .3)\n","        if verbose:\n","          print('\\nTotal score > 1, adjusting weights...') #debug statement, delete at end\n","          print('\\nSemantic similarity bonus : +', float(cs_bonus))\n","          print('Double word penalty:        -', double_word_penalty * .1,'\\n')\n","        grand_total = total + cs_bonus\n","        if grand_total < 1:   \n","          return grand_total\n","\n","        # bonus put score over 1   \n","        else:\n","          cs_bonus = get_bonus(cs, multiplier = .2)\n","          grand_total = total + cs_bonus\n","          if grand_total < 1:  \n","            return grand_total\n","          else:\n","            cs_bonus = get_bonus(cs, multiplier = .1)\n","            grand_total = total + cs_bonus\n","            if grand_total < 1:   \n","              return grand_total\n","\n","\n","\n","    # if no cs bonus      \n","    else:\n","      if verbose:\n","        print('\\nSemantic similarity bonus : +', float(cs_bonus))\n","        print('Double word penalty:        -', double_word_penalty * .1,'\\n')\n","      return total - (double_word_penalty * .1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS7Xb3UpDaWl"},"source":["Fix output contractions"]},{"cell_type":"code","metadata":{"id":"AnWfLYADDZrP"},"source":["# fix issues with contractions when displaying results.\n","# issues: 's' could represent possesion and not 'is.' Small fraction of the time though.\n","# inputs: two lists of words\n","# outputs: two lists of words\n","\n","def fix_contractions(ref,pred):\n","  for idx, word in enumerate(pred):\n","    if word == 're':\n","      pred[idx] = 'are'\n","    elif word == 'm':\n","      pred[idx] = 'am' \n","    elif word == 's':\n","      pred[idx] = 'is'   \n","    elif word == 'ok':\n","      pred[idx] = 'okay'  \n","    elif (word == 'aren' and pred[idx+1] == 't'):\n","      pred[idx] = 'are' \n","      pred[idx+1] = 'not'\n","    elif (word == 'isn' and pred[idx+1] == 't'):\n","      pred[idx] = 'is' \n","      pred[idx+1] = 'not'\n","    elif (word == 'don' and pred[idx+1] == 't'):\n","      pred[idx] = 'do' \n","      pred[idx+1] = 'not'\n","\n","  for idx, rword in enumerate(ref):\n","    if rword == 're':\n","      ref[idx] = 'are'\n","    elif rword == 'm':\n","      ref[idx] = 'am' \n","    elif rword == 'ok':\n","      ref[idx] = 'okay'       \n","    elif rword == 's':\n","      ref[idx] = 'is'  \n","    elif (rword == 'aren' and ref[idx+1] == 't'):\n","      ref[idx] = 'are' \n","      ref[idx+1] = 'not'        \n","    elif (rword == 'isn' and ref[idx+1] == 't'):\n","      ref[idx] = 'is' \n","      ref[idx+1] = 'not' \n","    elif (rword == 'don' and ref[idx+1] == 't'):\n","      ref[idx] = 'do' \n","      ref[idx+1] = 'not' \n","\n","  return ref, pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aZJaqVh_9VZQ"},"source":["Fix Punctuation"]},{"cell_type":"code","metadata":{"id":"MIFXJRCr2UpD"},"source":["# sometimes ending punctuation is filtered off prediction when no EOS token is predicted\n","# adding it back in to not trigger missed prediction\n","# input: two lists of words\n","# output: two lists of words\n","\n","def fix_punctuation(ref, pred):\n","  ending_punc = [ref[-1]]\n","  if pred[-1] not in ending_punc:\n","    pred.append(ending_punc[0])\n","  return ref, pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3Ghxzmg4OKx"},"source":["Initialize models and start training"]},{"cell_type":"code","metadata":{"id":"wQD807FacK7y"},"source":["hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","loss_tracker = trainIters(encoder1, decoder1, 250000, print_every=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S3uKpz0HRoqr"},"source":["Save loss for plotting"]},{"cell_type":"code","metadata":{"id":"w7fAtfXKJsWO"},"source":["with open('/content/scratch_wo_attn.pkl', 'wb') as f:\n","  pickle.dump(loss_tracker, f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"BfN36-uKOnKU","executionInfo":{"status":"ok","timestamp":1606777317752,"user_tz":300,"elapsed":688,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"549cb81c-eca8-4e9b-c805-0fab4a95eaf3"},"source":["from google.colab import files\n","files.download('/content/scratch_wo_attn.pkl') "],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_ffab5b83-e8a7-41d3-ba3d-18d1f734f252\", \"scratch_wo_attn.pkl\", 22512)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mbfGKl3_4XK2"},"source":["Load model weights from drive"]},{"cell_type":"code","metadata":{"id":"2V5wGhRvWvMn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d400513-347a-43b8-cb9f-1777da05d701"},"source":["#encoder1.load_state_dict(torch.load(encoder_weights_path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"5Nvb0-ZAWvI8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7553a020-f461-4fe8-e266-e485dd9491d8"},"source":["#attn_decoder1.load_state_dict(torch.load(decoder_weights_path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"AdMtVVmW4a7_"},"source":["Save weights after training"]},{"cell_type":"code","metadata":{"id":"uwzGKcwI3PiM"},"source":["#torch.save(encoder1.state_dict(), 'encoder1_scratch_woattn_0.0660.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CGd3Q_y3PlG"},"source":["#torch.save(decoder1.state_dict(), 'decoder1_scratch_woattn_0.0660.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oUGy4ra0kOS"},"source":["# w2v WE model for cosine sim.:\n","w2v_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/Embedding Models/word2vec.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"445Xch2s9pp3"},"source":["Calculate cosine similarities"]},{"cell_type":"code","metadata":{"id":"0BQHQocOlmY2"},"source":["# calculates penalties for double words, filters sentences to relevant words to compare,\n","# calculateds cos. sim., and a score for the strength of the cos. sims.\n","# inputs: two lists of words\n","# outputs: relevant word cosine sim. scores over 0.3, number of double words in the prediction\n","\n","def similarities(A,B,verbose=True):\n","\n","  # does B have more double words than A? \n","  doublesA = 0\n","  doublesB = 0\n","  basketA = []\n","  basketB = []\n","\n","  for i in A:\n","    if i not in basketA:\n","      basketA.append(i)\n","    else:\n","      doublesA += 1\n","\n","  for i in B: \n","    if i not in basketB:\n","      basketB.append(i)\n","    else:\n","      doublesB += 1\n","\n","  # calc penalty, keep only positive values\n","  double_word_penalty = np.clip(doublesB - doublesA, 0,3) \n","\n","  # get words not in the other sentence and not in punc/stopwords\n","  stop_words = ['a','an','of','the','to','on','t','in','as'] #,'not','no']\n","  punc = ['.','?','!',',']\n","  extraW = [] # all extra words\n","  extraA = []\n","  extraB = []\n","\n","  for i in A:\n","    if i not in punc:\n","      if i not in stop_words:\n","        if (i not in B):\n","          extraA.append(i)\n","  for i in B:\n","    if i not in punc:\n","      if i not in stop_words:\n","        if (i not in A):\n","          extraB.append(i)\n","\n","  extraW = extraA + extraB\n","\n","  # if off by one word, exit\n","  if len(extraW) == 1:\n","    return [0, double_word_penalty]\n","  \n","  # calc cos sims and score\n","  sim_finn = []\n","  sim_w2v = []\n","  record = []\n","\n","  for a, b in itertools.product(extraA,extraB):\n","    sim_w2v.append([a,b,w2v_model.similarity(a,b)])\n","\n","  sorted_sim_w2v = sorted(sim_w2v, key = lambda x: x[2], reverse=True)\n","\n","  cs_score = [0] #list of cs over 0.3\n","\n","  # print cs scores\n","  if sorted_sim_w2v:\n","    if verbose:\n","      print('\\nSemantic similarities using w2v:')\n","    for idx,i in enumerate(sorted_sim_w2v):\n","      if verbose:\n","        if i[2] > 0.3:\n","          print(bold, end=\"\")\n","          print('  ',i,reset)\n","        else:\n","          print('  ',i)\n","    #print('\\n')\n","    # record cs scores\n","    for i in sorted_sim_w2v:\n","      if i[2] > 0.3:\n","        cs_score.append(i[2])\n","\n","  return [cs_score, double_word_penalty]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YK_yUoalXfLb"},"source":["# run text through google translate api\n","# input: string\n","# output: string\n","\n","def google_translate(text):\n","  translator= Translator(from_lang=\"french\",to_lang=\"english\")\n","  translation = translator.translate(text)\n","  return translation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfFwBl_Ac1K_"},"source":["# evaluate n random pairs from dataset\n","# input: models, n\n","# output: none\n","\n","def evaluateAll(encoder, decoder, n=1000, all = False):\n","    print(bold+'Evaluation of Machine Translation Models'+reset)\n","    \n","    #scratch\n","    bleu_score_tracker = []\n","    gleu_score_tracker = []\n","    custom_tracker = []\n","\n","    #glove\n","    glove_bleu_score_tracker = []\n","    glove_gleu_score_tracker = []\n","    glove_custom_tracker = []\n","\n","    record_test = []\n","    ending_punc = ['.','?','!']\n","\n","    if not all: # run on n randomly chosen pairs\n","      print(bold+'Evaluating ',n,' examples...'+reset)\n","\n","      # get random indices to pull data from\n","      rand_indices = [random.randint(0,len(pairs)-1) for i in range(n)]\n","\n","      for i in rand_indices:\n","\n","          pair = pairs[i]\n","          # create lookup table for pairs including idx for easy look up\n","          output_words = evaluate(encoder, decoder, pair[0])  \n","          \n","          ###### preprocess function#######\n","          ref = pair[1].split()#[:-1]\n","          pred = output_words[:-1]\n","\n","          # fix contractions\n","          ref, pred = fix_contractions(ref, pred)\n","\n","          # if missing ending punctuation\n","          if pred[-1] not in ending_punc:\n","            ref, pred = fix_punctuation(ref, pred) \n","\n","          ###### look up other model pred function#######\n","          # get glove prediction\n","          glove = glove_frwac_df['Prediction'][i] #************\n","\n","          ###### get scores function#######\n","          bleu_one_gram = bleu([ref[:-1]],pred[:-1])\n","          glove_bleu = bleu([ref[:-1]],glove[:-1])#************\n","\n","          # DO NOT DISPLAY PERFECT SCORES - USED FOR EASY DEBUGGING - DELETE AT END\n","          # CONVERT TO DISPLAYING PERFECT SCORES SOME FRACTION OF THE TIME (1/5TH?)\n","          if bleu_one_gram < 1:\n","            print('\\n')\n","            print(bold+'Input:\\t'+reset, pair[0])\n","            print(bold+'Target:\\t'+reset, ' '.join(ref))\n","            print(bold+'S-Pred:\\t'+reset, ' '.join(pred))\n","            print(bold+'G-Pred:\\t'+reset, ' '.join(glove),'\\n')#************\n","            \n","            # requires ref to be a 2d list, pred 1d list\n","            bleu_score_tracker.append(bleu_one_gram)\n","            gleu_one_gram = gleu([ref[:-1]],pred[:-1])\n","            gleu_score_tracker.append(gleu_one_gram)\n","            print(f'Scratch Bleu Score: {bleu_one_gram:.3f}')\n","            print(f'Scratch Gleu Score: {gleu_one_gram:.3f}')\n","            print(f'Scratch Avg Score:  {(gleu_one_gram*.25+bleu_one_gram*.75):.3f}') #weighted\n","\n","            glove_bleu_score_tracker.append(glove_bleu)#************\n","            glove_gleu = gleu([ref[:-1]],glove[:-1])#************\n","            glove_gleu_score_tracker.append(glove_gleu)#************\n","            print(f'Glove Bleu Score: \\t{glove_bleu:.3f}')\n","            print(f'Glove Gleu Score: \\t{glove_gleu:.3f}')\n","            print(f'Glove Avg Score:  \\t{(glove_gleu*.25+glove_bleu*.75):.3f}') #weighted\n","            \n","\n","            \n","            cs_score = 0\n","\n","            # if not perfect score: calc. bonuses and penalties\n","            if bleu_one_gram < 1:\n","              try: # sometimes sims returns none\n","                sim_returns = similarities(ref,pred)\n","                cs_score = sim_returns[0]\n","                double_word_penalty = sim_returns[1]\n","                cust_score = custom_score(bleu_one_gram,gleu_one_gram,cs_score,double_word_penalty)\n","                print(f'{bold_red_font_tag}S-Custom Score: {cust_score:.3f}{reset}')\n","\n","              # if word not in WE\n","              except KeyError:\n","                print('Cosine similarities: Word not found in embedding vocabulary')\n","                continue\n","            else:\n","              cust_score = custom_score(bleu_one_gram,gleu_one_gram,0,0)\n","              print(f'{bold_red_font_tag}S-Custom Score: {cust_score:.3f}{reset}')\n","\n","            custom_tracker.append(cust_score)\n","\n","\n","            if glove_bleu < 1:\n","              try: # sometimes sims returns none\n","                glove_sim_returns = similarities(ref,glove)\n","                glove_cs_score = glove_sim_returns[0]\n","                glove_double_word_penalty = glove_sim_returns[1]\n","                glove_cust_score = custom_score(glove_bleu,glove_gleu,glove_cs_score,glove_double_word_penalty)\n","                print(f'{bold_red_font_tag}G-Custom Score: {glove_cust_score:.3f}{reset}')\n","                #print('\\n')\n","\n","              # if word not in WE\n","              except KeyError:\n","                print('Cosine similarities: Word not found in embedding vocabulary')\n","                continue\n","            else:\n","              glove_cust_score = custom_score(glove_bleu,glove_gleu,0,0)\n","              print(f'{bold_red_font_tag}G-Custom Score: {glove_cust_score:.3f}{reset}')\n","\n","            glove_custom_tracker.append(glove_cust_score)\n","\n","\n","      print('\\n')\n","      print(f'{bold_blue_font_tag}Avg S-Bleu Score  :{reset} {sum(bleu_score_tracker)/len(bleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg S-Gleu Score  :{reset} {sum(gleu_score_tracker)/len(gleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg S-Custom Score:{reset} {sum(custom_tracker)/len(custom_tracker):.3f}')\n","\n","      print('\\n')\n","      print(f'{bold_blue_font_tag}Avg G-Bleu Score  :{reset} {sum(glove_bleu_score_tracker)/len(glove_bleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg G-Gleu Score  :{reset} {sum(glove_gleu_score_tracker)/len(glove_gleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg G-Custom Score:{reset} {sum(glove_custom_tracker)/len(glove_custom_tracker):.3f}')\n","\n","    else: # run on entire dataset\n","      print(bold+'Evaluating entire dataset...'+reset)\n","      for i in range(len(pairs)):\n","        pair = pairs[i]\n","        output_words = evaluate(encoder, decoder, pair[0])  \n","        ref = pair[1].split()#[:-1]\n","        pred = output_words[:-1]\n","\n","        # fix contractions\n","        ref, pred = fix_contractions(ref, pred)\n","\n","        # if missing ending punctuation\n","        if pred[-1] not in ending_punc:\n","          ref, pred = fix_punctuation(ref, pred) \n","\n","        bleu_one_gram = bleu([ref[:-1]],pred[:-1])\n","        bleu_score_tracker.append(bleu_one_gram)\n","        gleu_one_gram = gleu([ref[:-1]],pred[:-1])\n","        gleu_score_tracker.append(gleu_one_gram)\n","\n","        cs_score = 0\n","\n","        # if not perfect score: calc. bonuses and penalties\n","        if bleu_one_gram < 1:\n","          try: # sometimes sims returns none\n","            sim_returns = similarities(ref,pred, verbose=False)\n","            cs_score = sim_returns[0]\n","            double_word_penalty = sim_returns[1]\n","            cust_score = custom_score(bleu_one_gram,gleu_one_gram,cs_score,double_word_penalty, verbose=False)\n","            #print(bold_red_font_tag+'Custom Score: ',cust_score,reset)\n","            #print('\\n')\n","\n","          # if word not in WE\n","          except KeyError:\n","            #print('Cosine similarities: Word not found in embedding vocabulary')\n","            continue\n","        else:\n","          cust_score = custom_score(bleu_one_gram,gleu_one_gram,0,0, verbose=False)\n","          #print(bold_red_font_tag+'Custom Score: ',cust_score,reset)\n","\n","        custom_tracker.append(cust_score)\n","\n","      print('\\n')\n","      print(f'{bold_blue_font_tag}Avg Bleu Score  :{reset} {sum(bleu_score_tracker)/len(bleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg Gleu Score  :{reset} {sum(gleu_score_tracker)/len(gleu_score_tracker):.3f}')\n","      print(f'{bold_blue_font_tag}Avg Custom Score:{reset} {sum(custom_tracker)/len(custom_tracker):.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpNYuSvIRd9n"},"source":["Get predictions from other models"]},{"cell_type":"code","metadata":{"id":"e3Ja6XtKcHjO"},"source":["with open('/content/drive/MyDrive/Embedding Models/glove_frwac_results.pkl', 'rb') as f:\n","  glove_frwac_results = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Embedding Models/ft_frwac_results_V1.pkl', 'rb') as f:\n","  ft_frwac_results = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/Embedding Models/w2v_frwac_results_V2.pkl', 'rb') as f:\n","  w2v_frwac_results = pickle.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49wphMiHcU-V"},"source":["# Idx is the index of the sentence in the pairs dataset\n","\n","import pandas as pd\n","glove_frwac_df = pd.DataFrame(glove_frwac_results, columns=['Idx','Ref','Prediction'])\n","#glove_frwac_df.sample(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODdU1IfYS_lY"},"source":["w2v_frwac_df = pd.DataFrame(w2v_frwac_results, columns=['Idx','Ref','Prediction'])\n","#w2v_frwac_df.sample(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39bPlfHlS_gY"},"source":["ft_frwac_df = pd.DataFrame(ft_frwac_results, columns=['Idx','Ref','Prediction'])\n","#ft_frwac_df.sample(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnE8noUTR6WE"},"source":["Evaluate n random pairs"]},{"cell_type":"code","metadata":{"id":"WbpDYJijNp78"},"source":["# evaluate n random pairs from dataset\n","# input: models, n\n","# output: none\n","\n","def evaluateRandomlySimplifiedOutput(encoder, decoder, n=1000):\n","    print(bold+'Evaluation of Machine Translation Models'+reset)\n","    \n","    #scratch\n","    bleu_score_tracker = []\n","    gleu_score_tracker = []\n","    custom_tracker = []\n","\n","    #glove\n","    glove_bleu_score_tracker = []\n","    glove_gleu_score_tracker = []\n","    glove_custom_tracker = []\n","\n","    #w2v\n","    w2v_bleu_score_tracker = []\n","    w2v_gleu_score_tracker = []\n","    w2v_custom_tracker = []\n","\n","    #ft\n","    ft_bleu_score_tracker = []\n","    ft_gleu_score_tracker = []\n","    ft_custom_tracker = []\n","\n","    record_test = []\n","    ending_punc = ['.','?','!']\n","\n","    print(bold+'Evaluating ',n,' examples...'+reset)\n","\n","    # get random indices to pull data from\n","    rand_indices = [random.randint(0,len(pairs)-1) for i in range(n)]\n","\n","    for i in rand_indices:\n","\n","        pair = pairs[i]\n","        # create lookup table for pairs including idx for easy look up\n","        output_words = evaluate(encoder, decoder, pair[0])  \n","        \n","        ###### preprocess function#######\n","        ref = pair[1].split()#[:-1]\n","        pred = output_words[:-1]\n","\n","        # fix contractions\n","        ref, pred = fix_contractions(ref, pred)\n","\n","        # if missing ending punctuation\n","        if pred[-1] not in ending_punc:\n","          ref, pred = fix_punctuation(ref, pred) \n","\n","        ###### look up other model pred function#######\n","        # get glove prediction\n","        glove = glove_frwac_df['Prediction'][i] #************\n","\n","        # get w2v prediction\n","        w2v = w2v_frwac_df['Prediction'][i] #************\n","\n","        # get fasttext prediction\n","        ft = ft_frwac_df['Prediction'][i] #************\n","\n","\n","        ###### get scores function#######\n","        bleu_one_gram = bleu([ref[:-1]],pred[:-1])\n","        glove_bleu = bleu([ref[:-1]],glove[:-1])#************\n","        w2v_bleu = bleu([ref[:-1]],w2v[:-1])#************\n","        ft_bleu = bleu([ref[:-1]],ft[:-1])#************\n","\n","\n","        # DO NOT DISPLAY PERFECT SCORES - USED FOR EASY DEBUGGING - DELETE AT END\n","        # CONVERT TO DISPLAYING PERFECT SCORES SOME FRACTION OF THE TIME (1/5TH?)\n","        if bleu_one_gram < 1:\n","          print('\\n')\n","          print(bold+'Input:\\t'+reset, pair[0])\n","          print(bold+'Target:\\t'+reset, ' '.join(ref),'\\n')\n","\n","          print(bold+'G-Pred:\\t'+reset, ' '.join(glove))#************\n","          print(bold+'W-Pred:\\t'+reset, ' '.join(w2v))#************\n","          print(bold+'F-Pred:\\t'+reset, ' '.join(ft))#************\n","          print(bold+'S-Pred:\\t'+reset, ' '.join(pred),'\\n')\n","\n","          # requires ref to be a 2d list, pred 1d list\n","          bleu_score_tracker.append(bleu_one_gram)\n","          gleu_one_gram = gleu([ref[:-1]],pred[:-1])\n","          gleu_score_tracker.append(gleu_one_gram)\n","\n","          glove_bleu_score_tracker.append(glove_bleu)#************\n","          glove_gleu = gleu([ref[:-1]],glove[:-1])#************\n","          glove_gleu_score_tracker.append(glove_gleu)#************\n","\n","          w2v_bleu_score_tracker.append(w2v_bleu)#************\n","          w2v_gleu = gleu([ref[:-1]],w2v[:-1])#************\n","          w2v_gleu_score_tracker.append(w2v_gleu)#************\n","\n","          ft_bleu_score_tracker.append(ft_bleu)#************\n","          ft_gleu = gleu([ref[:-1]],ft[:-1])#************\n","          ft_gleu_score_tracker.append(ft_gleu)#************\n","\n","          cs_score = 0\n","\n","          # if not perfect score: calc. bonuses and penalties\n","          if bleu_one_gram < 1:\n","            try: # sometimes sims returns none\n","              sim_returns = similarities(ref,pred,verbose=False)\n","              cs_score = sim_returns[0]\n","              double_word_penalty = sim_returns[1]\n","              cust_score = custom_score(bleu_one_gram,gleu_one_gram,cs_score,double_word_penalty,verbose=False)\n","\n","            # if word not in WE\n","            except KeyError:\n","              continue\n","          else:\n","            cust_score = custom_score(bleu_one_gram,gleu_one_gram,0,0,verbose=False)\n","\n","          custom_tracker.append(cust_score)\n","\n","          # glove\n","          if bleu_one_gram < 1:\n","            try: # sometimes sims returns none\n","              glove_sim_returns = similarities(ref,glove,verbose=False)\n","              glove_cs_score = glove_sim_returns[0]\n","              glove_double_word_penalty = glove_sim_returns[1]\n","              glove_cust_score = custom_score(glove_bleu,glove_gleu,glove_cs_score,glove_double_word_penalty,verbose=False)\n","\n","            # if word not in WE\n","            except KeyError:\n","              continue\n","          else:\n","            glove_cust_score = custom_score(glove_bleu,glove_gleu,0,0,verbose=False)\n","\n","          glove_custom_tracker.append(glove_cust_score)\n","\n","          # w2v\n","          if bleu_one_gram < 1:\n","            try: # sometimes sims returns none\n","              w2v_sim_returns = similarities(ref,w2v,verbose=False)\n","              w2v_cs_score = w2v_sim_returns[0]\n","              w2v_double_word_penalty = w2v_sim_returns[1]\n","              w2v_cust_score = custom_score(w2v_bleu,w2v_gleu,w2v_cs_score,w2v_double_word_penalty,verbose=False)\n","\n","            # if word not in WE\n","            except KeyError:\n","              continue\n","          else:\n","            w2v_cust_score = custom_score(w2v_bleu,w2v_gleu,0,0,verbose=False)\n","\n","          w2v_custom_tracker.append(w2v_cust_score)\n","\n","          # ft\n","          if bleu_one_gram < 1:\n","            try: # sometimes sims returns none\n","              ft_sim_returns = similarities(ref,ft,verbose=False)\n","              ft_cs_score = ft_sim_returns[0]\n","              ft_double_word_penalty = ft_sim_returns[1]\n","              ft_cust_score = custom_score(ft_bleu,ft_gleu,ft_cs_score,ft_double_word_penalty,verbose=False)\n","\n","            # if word not in WE\n","            except KeyError:\n","              continue\n","          else:\n","            ft_cust_score = custom_score(ft_bleu,ft_gleu,0,0,verbose=False)\n","\n","          ft_custom_tracker.append(ft_cust_score)\n","\n","          print(f'G-Scores:\\tBleu: {glove_bleu:.3f}\\tGleu: {glove_gleu:.3f}\\tCustom Score: {glove_cust_score:.3f}')\n","          print(f'W-Scores:\\tBleu: {w2v_bleu:.3f}\\tGleu: {w2v_gleu:.3f}\\tCustom Score: {w2v_cust_score:.3f}')\n","          print(f'F-Scores:\\tBleu: {ft_bleu:.3f}\\tGleu: {ft_gleu:.3f}\\tCustom Score: {ft_cust_score:.3f}')\n","          print(f'S-Scores:\\tBleu: {bleu_one_gram:.3f}\\tGleu: {gleu_one_gram:.3f}\\tCustom Score: {cust_score:.3f}')\n","\n","    print('\\n')\n","    print(f'{bold_blue_font_tag}Avg G-Bleu Score  :{reset} {sum(glove_bleu_score_tracker)/len(glove_bleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg G-Gleu Score  :{reset} {sum(glove_gleu_score_tracker)/len(glove_gleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg G-Custom Score:{reset} {sum(glove_custom_tracker)/len(glove_custom_tracker):.3f}')\n","\n","    print('\\n')\n","    print(f'{bold_blue_font_tag}Avg W-Bleu Score  :{reset} {sum(w2v_bleu_score_tracker)/len(w2v_bleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg W-Gleu Score  :{reset} {sum(w2v_gleu_score_tracker)/len(w2v_gleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg W-Custom Score:{reset} {sum(w2v_custom_tracker)/len(w2v_custom_tracker):.3f}')\n","\n","    print('\\n')\n","    print(f'{bold_blue_font_tag}Avg F-Bleu Score  :{reset} {sum(ft_bleu_score_tracker)/len(ft_bleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg F-Gleu Score  :{reset} {sum(ft_gleu_score_tracker)/len(ft_gleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg F-Custom Score:{reset} {sum(ft_custom_tracker)/len(ft_custom_tracker):.3f}')\n","\n","    print('\\n')\n","    print(f'{bold_blue_font_tag}Avg S-Bleu Score  :{reset} {sum(bleu_score_tracker)/len(bleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg S-Gleu Score  :{reset} {sum(gleu_score_tracker)/len(gleu_score_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg S-Custom Score:{reset} {sum(custom_tracker)/len(custom_tracker):.3f}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Px50X-DQSAbO"},"source":["Compare all models on n random samples of dataset  \n","Only displaying examples were learned emb model score was < 1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqIY6QNoXKJz","executionInfo":{"status":"ok","timestamp":1606780344850,"user_tz":300,"elapsed":9315,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"2de9ead1-ad14-4ce3-8a63-35f4cfec07c6"},"source":["evaluateRandomlySimplifiedOutput(encoder1, decoder1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1mEvaluation of Machine Translation Models\u001b[0m\n","\u001b[1mEvaluating  1000  examples...\u001b[0m\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas encore pret .\n","\u001b[1mTarget:\t\u001b[0m i am not ready yet . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not ready yet .\n","\u001b[1mW-Pred:\t\u001b[0m i am not proud of this .\n","\u001b[1mF-Pred:\t\u001b[0m i am not ready yet .\n","\u001b[1mS-Pred:\t\u001b[0m i am not not ready yet . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.500\tGleu: 0.333\tCustom Score: 0.591\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.833\tGleu: 0.611\tCustom Score: 0.678\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis malade .\n","\u001b[1mTarget:\t\u001b[0m i am ill . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am ill .\n","\u001b[1mW-Pred:\t\u001b[0m i am impatient .\n","\u001b[1mF-Pred:\t\u001b[0m i am ill .\n","\u001b[1mS-Pred:\t\u001b[0m i am sick . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.886\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes tres elegants .\n","\u001b[1mTarget:\t\u001b[0m you are very sophisticated . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are very stylish .\n","\u001b[1mW-Pred:\t\u001b[0m you are very funny .\n","\u001b[1mF-Pred:\t\u001b[0m you are very sophisticated .\n","\u001b[1mS-Pred:\t\u001b[0m you are very stylish . \n","\n","G-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.863\n","W-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.713\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.863\n","\n","\n","\u001b[1mInput:\t\u001b[0m t es marrant .\n","\u001b[1mTarget:\t\u001b[0m you are fun . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are funny .\n","\u001b[1mW-Pred:\t\u001b[0m you are funny .\n","\u001b[1mF-Pred:\t\u001b[0m you are funny .\n","\u001b[1mS-Pred:\t\u001b[0m you are funny . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.856\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.856\n","F-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.856\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.856\n","\n","\n","\u001b[1mInput:\t\u001b[0m je vais bien .\n","\u001b[1mTarget:\t\u001b[0m i am well . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am just kidding .\n","\u001b[1mW-Pred:\t\u001b[0m i am okay .\n","\u001b[1mF-Pred:\t\u001b[0m i am going to be happy .\n","\u001b[1mS-Pred:\t\u001b[0m i am okay . \n","\n","G-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.590\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.749\n","F-Scores:\tBleu: 0.333\tGleu: 0.167\tCustom Score: 0.292\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.749\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous n etes pas ma mere .\n","\u001b[1mTarget:\t\u001b[0m you are not my mother . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are not my mother .\n","\u001b[1mW-Pred:\t\u001b[0m you are not dead .\n","\u001b[1mF-Pred:\t\u001b[0m you are not my mother .\n","\u001b[1mS-Pred:\t\u001b[0m you are t my mother . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.584\tGleu: 0.429\tCustom Score: 0.545\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.800\tGleu: 0.429\tCustom Score: 0.707\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas ta servante .\n","\u001b[1mTarget:\t\u001b[0m i am not your servant . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not your servant .\n","\u001b[1mW-Pred:\t\u001b[0m i am not your enemy .\n","\u001b[1mF-Pred:\t\u001b[0m i am not your maid .\n","\u001b[1mS-Pred:\t\u001b[0m i am not your maid . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.800\tGleu: 0.714\tCustom Score: 0.779\n","F-Scores:\tBleu: 0.800\tGleu: 0.714\tCustom Score: 0.931\n","S-Scores:\tBleu: 0.800\tGleu: 0.714\tCustom Score: 0.931\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas votre ami .\n","\u001b[1mTarget:\t\u001b[0m i am not your friend . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not your friend .\n","\u001b[1mW-Pred:\t\u001b[0m i am not your enemy .\n","\u001b[1mF-Pred:\t\u001b[0m i am not your daughter .\n","\u001b[1mS-Pred:\t\u001b[0m i am no friend friend . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.800\tGleu: 0.714\tCustom Score: 0.779\n","F-Scores:\tBleu: 0.800\tGleu: 0.714\tCustom Score: 0.990\n","S-Scores:\tBleu: 0.600\tGleu: 0.286\tCustom Score: 0.729\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu as parfaitement raison .\n","\u001b[1mTarget:\t\u001b[0m you are completely right . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are perfectly right .\n","\u001b[1mW-Pred:\t\u001b[0m you are absolutely right .\n","\u001b[1mF-Pred:\t\u001b[0m you are absolutely right .\n","\u001b[1mS-Pred:\t\u001b[0m you are absolutely right . \n","\n","G-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.837\n","W-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.868\n","F-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.868\n","S-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.868\n","\n","\n","\u001b[1mInput:\t\u001b[0m ca va pas le faire .\n","\u001b[1mTarget:\t\u001b[0m we are not gonna make it . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are not turning back .\n","\u001b[1mW-Pred:\t\u001b[0m we are not going to make it .\n","\u001b[1mF-Pred:\t\u001b[0m we are not going to make it .\n","\u001b[1mS-Pred:\t\u001b[0m we are not gonna to it it . \n","\n","G-Scores:\tBleu: 0.491\tGleu: 0.333\tCustom Score: 0.580\n","W-Scores:\tBleu: 0.714\tGleu: 0.409\tCustom Score: 0.936\n","F-Scores:\tBleu: 0.714\tGleu: 0.409\tCustom Score: 0.936\n","S-Scores:\tBleu: 0.714\tGleu: 0.500\tCustom Score: 0.561\n","\n","\n","\u001b[1mInput:\t\u001b[0m je n en suis pas sur .\n","\u001b[1mTarget:\t\u001b[0m i am not sure about that . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not sure about it .\n","\u001b[1mW-Pred:\t\u001b[0m i am not sure .\n","\u001b[1mF-Pred:\t\u001b[0m i am not afraid to make the mistake .\n","\u001b[1mS-Pred:\t\u001b[0m i am not sure about it . \n","\n","G-Scores:\tBleu: 0.833\tGleu: 0.778\tCustom Score: 0.955\n","W-Scores:\tBleu: 0.607\tGleu: 0.556\tCustom Score: 0.594\n","F-Scores:\tBleu: 0.375\tGleu: 0.231\tCustom Score: 0.798\n","S-Scores:\tBleu: 0.833\tGleu: 0.778\tCustom Score: 0.955\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne vais pas m impliquer .\n","\u001b[1mTarget:\t\u001b[0m i am not going to get involved . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not getting involved .\n","\u001b[1mW-Pred:\t\u001b[0m i am not going to stop .\n","\u001b[1mF-Pred:\t\u001b[0m i am not going to get involved .\n","\u001b[1mS-Pred:\t\u001b[0m i am not getting involved . \n","\n","G-Scores:\tBleu: 0.536\tGleu: 0.318\tCustom Score: 0.989\n","W-Scores:\tBleu: 0.705\tGleu: 0.636\tCustom Score: 0.688\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.536\tGleu: 0.318\tCustom Score: 0.989\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis toujours occupe .\n","\u001b[1mTarget:\t\u001b[0m i am still busy . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not a fool .\n","\u001b[1mW-Pred:\t\u001b[0m i am just curious .\n","\u001b[1mF-Pred:\t\u001b[0m i am too busy to help him .\n","\u001b[1mS-Pred:\t\u001b[0m i am busy busy time . \n","\n","G-Scores:\tBleu: 0.400\tGleu: 0.214\tCustom Score: 0.540\n","W-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.642\n","F-Scores:\tBleu: 0.429\tGleu: 0.182\tCustom Score: 0.552\n","S-Scores:\tBleu: 0.600\tGleu: 0.286\tCustom Score: 0.421\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas fatiguee du tout .\n","\u001b[1mTarget:\t\u001b[0m i am not at all tired . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not tired at all .\n","\u001b[1mW-Pred:\t\u001b[0m i am not done .\n","\u001b[1mF-Pred:\t\u001b[0m i am not tired yet .\n","\u001b[1mS-Pred:\t\u001b[0m i am not at at tired . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 0.556\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.455\tGleu: 0.333\tCustom Score: 0.425\n","F-Scores:\tBleu: 0.655\tGleu: 0.389\tCustom Score: 0.588\n","S-Scores:\tBleu: 0.833\tGleu: 0.611\tCustom Score: 0.678\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous ne sommes pas maries .\n","\u001b[1mTarget:\t\u001b[0m we are not married . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are not married .\n","\u001b[1mW-Pred:\t\u001b[0m we are not open .\n","\u001b[1mF-Pred:\t\u001b[0m we are not married .\n","\u001b[1mS-Pred:\t\u001b[0m we aren not married . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.713\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.852\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous sommes dans le petrin .\n","\u001b[1mTarget:\t\u001b[0m we are up a creek without a paddle . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are in trouble .\n","\u001b[1mW-Pred:\t\u001b[0m we are starving .\n","\u001b[1mF-Pred:\t\u001b[0m we are all tired of it .\n","\u001b[1mS-Pred:\t\u001b[0m we are up trouble . \n","\n","G-Scores:\tBleu: 0.184\tGleu: 0.115\tCustom Score: 0.167\n","W-Scores:\tBleu: 0.126\tGleu: 0.115\tCustom Score: 0.123\n","F-Scores:\tBleu: 0.239\tGleu: 0.115\tCustom Score: 0.208\n","S-Scores:\tBleu: 0.276\tGleu: 0.231\tCustom Score: 0.265\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis enchante de vous rencontrer .\n","\u001b[1mTarget:\t\u001b[0m i am delighted to meet you . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mW-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mF-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mS-Pred:\t\u001b[0m i am charmed to meet you . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.833\tGleu: 0.500\tCustom Score: 0.750\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle est non seulement belle mais aussi intelligente .\n","\u001b[1mTarget:\t\u001b[0m she is not only beautiful but also intelligent . \n","\n","\u001b[1mG-Pred:\t\u001b[0m she is not a only smart girl .\n","\u001b[1mW-Pred:\t\u001b[0m she is more older than him .\n","\u001b[1mF-Pred:\t\u001b[0m she is not a beautiful sheep .\n","\u001b[1mS-Pred:\t\u001b[0m she is not only intelligent but also intelligent . \n","\n","G-Scores:\tBleu: 0.495\tGleu: 0.269\tCustom Score: 0.699\n","W-Scores:\tBleu: 0.239\tGleu: 0.115\tCustom Score: 0.742\n","F-Scores:\tBleu: 0.478\tGleu: 0.269\tCustom Score: 0.426\n","S-Scores:\tBleu: 0.875\tGleu: 0.615\tCustom Score: 0.710\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes sympa .\n","\u001b[1mTarget:\t\u001b[0m you are nice . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are nice .\n","\u001b[1mW-Pred:\t\u001b[0m you are courageous .\n","\u001b[1mF-Pred:\t\u001b[0m you are funny .\n","\u001b[1mS-Pred:\t\u001b[0m you are cool . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","F-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.822\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.824\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle est l editeur en chef .\n","\u001b[1mTarget:\t\u001b[0m she is the executive editor . \n","\n","\u001b[1mG-Pred:\t\u001b[0m she is the editor in chief .\n","\u001b[1mW-Pred:\t\u001b[0m she is connected with that company .\n","\u001b[1mF-Pred:\t\u001b[0m she is a carbon painter .\n","\u001b[1mS-Pred:\t\u001b[0m she is the editor in . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.389\tCustom Score: 0.798\n","W-Scores:\tBleu: 0.333\tGleu: 0.167\tCustom Score: 0.292\n","F-Scores:\tBleu: 0.400\tGleu: 0.214\tCustom Score: 0.354\n","S-Scores:\tBleu: 0.800\tGleu: 0.500\tCustom Score: 0.725\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne l invente pas .\n","\u001b[1mTarget:\t\u001b[0m i am not making it up . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not making any plans .\n","\u001b[1mW-Pred:\t\u001b[0m i am not hungry .\n","\u001b[1mF-Pred:\t\u001b[0m i am not talking to parties .\n","\u001b[1mS-Pred:\t\u001b[0m i am not making that up . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.556\tCustom Score: 0.794\n","W-Scores:\tBleu: 0.455\tGleu: 0.333\tCustom Score: 0.425\n","F-Scores:\tBleu: 0.500\tGleu: 0.333\tCustom Score: 0.458\n","S-Scores:\tBleu: 0.833\tGleu: 0.611\tCustom Score: 0.981\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle a cinq ans de moins que moi .\n","\u001b[1mTarget:\t\u001b[0m she is five years younger than i am . \n","\n","\u001b[1mG-Pred:\t\u001b[0m she is two years younger than me .\n","\u001b[1mW-Pred:\t\u001b[0m she is devoted to meet you .\n","\u001b[1mF-Pred:\t\u001b[0m she is five years younger than me .\n","\u001b[1mS-Pred:\t\u001b[0m she is five years younger than me . \n","\n","G-Scores:\tBleu: 0.619\tGleu: 0.346\tCustom Score: 0.896\n","W-Scores:\tBleu: 0.239\tGleu: 0.115\tCustom Score: 0.361\n","F-Scores:\tBleu: 0.743\tGleu: 0.692\tCustom Score: 0.854\n","S-Scores:\tBleu: 0.743\tGleu: 0.692\tCustom Score: 0.854\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est en train de lire un roman .\n","\u001b[1mTarget:\t\u001b[0m he is reading a novel now . \n","\n","\u001b[1mG-Pred:\t\u001b[0m he is in his class .\n","\u001b[1mW-Pred:\t\u001b[0m he is on the radio .\n","\u001b[1mF-Pred:\t\u001b[0m he is reading a novel .\n","\u001b[1mS-Pred:\t\u001b[0m he is reading a novel right now . \n","\n","G-Scores:\tBleu: 0.327\tGleu: 0.167\tCustom Score: 0.287\n","W-Scores:\tBleu: 0.327\tGleu: 0.167\tCustom Score: 0.287\n","F-Scores:\tBleu: 0.819\tGleu: 0.778\tCustom Score: 0.808\n","S-Scores:\tBleu: 0.857\tGleu: 0.682\tCustom Score: 0.813\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous sommes attaquees .\n","\u001b[1mTarget:\t\u001b[0m we are being attacked . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are under attack .\n","\u001b[1mW-Pred:\t\u001b[0m we are being attacked .\n","\u001b[1mF-Pred:\t\u001b[0m we are a couple .\n","\u001b[1mS-Pred:\t\u001b[0m we are under attack . \n","\n","G-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.672\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.450\n","S-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.672\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis content de ma nouvelle veste .\n","\u001b[1mTarget:\t\u001b[0m i am pleased with my new jacket . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am pleased with his job .\n","\u001b[1mW-Pred:\t\u001b[0m i am going to be your teacher .\n","\u001b[1mF-Pred:\t\u001b[0m i am pretty busy is waiting .\n","\u001b[1mS-Pred:\t\u001b[0m i am pleased with my new bathing suit . \n","\n","G-Scores:\tBleu: 0.564\tGleu: 0.455\tCustom Score: 0.780\n","W-Scores:\tBleu: 0.286\tGleu: 0.136\tCustom Score: 0.647\n","F-Scores:\tBleu: 0.282\tGleu: 0.136\tCustom Score: 0.246\n","S-Scores:\tBleu: 0.750\tGleu: 0.692\tCustom Score: 0.860\n","\n","\n","\u001b[1mInput:\t\u001b[0m c est moi le patron ici .\n","\u001b[1mTarget:\t\u001b[0m i am in charge here . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am the boss around here .\n","\u001b[1mW-Pred:\t\u001b[0m i am fine into all .\n","\u001b[1mF-Pred:\t\u001b[0m i am the boss here .\n","\u001b[1mS-Pred:\t\u001b[0m i am the boss around here . \n","\n","G-Scores:\tBleu: 0.500\tGleu: 0.222\tCustom Score: 0.431\n","W-Scores:\tBleu: 0.400\tGleu: 0.214\tCustom Score: 0.505\n","F-Scores:\tBleu: 0.600\tGleu: 0.286\tCustom Score: 0.521\n","S-Scores:\tBleu: 0.500\tGleu: 0.222\tCustom Score: 0.431\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu es tres intelligent .\n","\u001b[1mTarget:\t\u001b[0m you are very intelligent . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are very intelligent .\n","\u001b[1mW-Pred:\t\u001b[0m you are very intelligent .\n","\u001b[1mF-Pred:\t\u001b[0m you are very smart .\n","\u001b[1mS-Pred:\t\u001b[0m you are very smart . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.972\n","S-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.972\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu es fiable .\n","\u001b[1mTarget:\t\u001b[0m you are trustworthy . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are reliable .\n","\u001b[1mW-Pred:\t\u001b[0m you are creative .\n","\u001b[1mF-Pred:\t\u001b[0m you are finicky .\n","\u001b[1mS-Pred:\t\u001b[0m you are reliable . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.860\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","F-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.860\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est gentil .\n","\u001b[1mTarget:\t\u001b[0m he is a kind person . \n","\n","\u001b[1mG-Pred:\t\u001b[0m he is kind .\n","\u001b[1mW-Pred:\t\u001b[0m he is very learned .\n","\u001b[1mF-Pred:\t\u001b[0m he is a smart world .\n","\u001b[1mS-Pred:\t\u001b[0m he is kind . \n","\n","G-Scores:\tBleu: 0.513\tGleu: 0.286\tCustom Score: 0.456\n","W-Scores:\tBleu: 0.389\tGleu: 0.214\tCustom Score: 0.512\n","F-Scores:\tBleu: 0.600\tGleu: 0.429\tCustom Score: 0.557\n","S-Scores:\tBleu: 0.513\tGleu: 0.286\tCustom Score: 0.456\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis en retard .\n","\u001b[1mTarget:\t\u001b[0m i am late . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am early .\n","\u001b[1mW-Pred:\t\u001b[0m i am eating now .\n","\u001b[1mF-Pred:\t\u001b[0m i am in way my light .\n","\u001b[1mS-Pred:\t\u001b[0m i am behind . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.950\n","W-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.450\n","F-Scores:\tBleu: 0.333\tGleu: 0.167\tCustom Score: 0.292\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas convaincu .\n","\u001b[1mTarget:\t\u001b[0m i am not persuaded . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not convinced .\n","\u001b[1mW-Pred:\t\u001b[0m i am not ugly .\n","\u001b[1mF-Pred:\t\u001b[0m i am not persuaded .\n","\u001b[1mS-Pred:\t\u001b[0m i am not convinced . \n","\n","G-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.964\n","W-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.713\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.964\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu es deux fois plus fort que moi .\n","\u001b[1mTarget:\t\u001b[0m you are twice as strong as i am . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are twice as strong as i am .\n","\u001b[1mW-Pred:\t\u001b[0m you are twice as strong as me .\n","\u001b[1mF-Pred:\t\u001b[0m you are twice as strong as me .\n","\u001b[1mS-Pred:\t\u001b[0m you are twice as strong as me . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.743\tGleu: 0.692\tCustom Score: 0.854\n","F-Scores:\tBleu: 0.743\tGleu: 0.692\tCustom Score: 0.854\n","S-Scores:\tBleu: 0.743\tGleu: 0.692\tCustom Score: 0.854\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis enchante de vous rencontrer .\n","\u001b[1mTarget:\t\u001b[0m i am delighted to meet you . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mW-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mF-Pred:\t\u001b[0m i am delighted to meet you .\n","\u001b[1mS-Pred:\t\u001b[0m i am charmed to meet you . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.833\tGleu: 0.500\tCustom Score: 0.750\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu es fou .\n","\u001b[1mTarget:\t\u001b[0m you are mad . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are demented .\n","\u001b[1mW-Pred:\t\u001b[0m you are charming .\n","\u001b[1mF-Pred:\t\u001b[0m you are silly .\n","\u001b[1mS-Pred:\t\u001b[0m you are crazy . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.816\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.625\n","F-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.819\n","S-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.920\n","\n","\n","\u001b[1mInput:\t\u001b[0m j en ai fini .\n","\u001b[1mTarget:\t\u001b[0m i am done with it . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am through .\n","\u001b[1mW-Pred:\t\u001b[0m i am done with it .\n","\u001b[1mF-Pred:\t\u001b[0m i am finished careful .\n","\u001b[1mS-Pred:\t\u001b[0m i am done . \n","\n","G-Scores:\tBleu: 0.342\tGleu: 0.214\tCustom Score: 0.453\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 0.389\tGleu: 0.214\tCustom Score: 0.493\n","S-Scores:\tBleu: 0.513\tGleu: 0.429\tCustom Score: 0.492\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle est son amie .\n","\u001b[1mTarget:\t\u001b[0m she is his friend . \n","\n","\u001b[1mG-Pred:\t\u001b[0m she is her friend .\n","\u001b[1mW-Pred:\t\u001b[0m she is a gifted artist .\n","\u001b[1mF-Pred:\t\u001b[0m she is a real gossip .\n","\u001b[1mS-Pred:\t\u001b[0m she is her friend . \n","\n","G-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.917\n","W-Scores:\tBleu: 0.400\tGleu: 0.214\tCustom Score: 0.354\n","F-Scores:\tBleu: 0.400\tGleu: 0.214\tCustom Score: 0.354\n","S-Scores:\tBleu: 0.750\tGleu: 0.400\tCustom Score: 0.917\n","\n","\n","\u001b[1mInput:\t\u001b[0m tu es idiote .\n","\u001b[1mTarget:\t\u001b[0m you are stupid . \n","\n","\u001b[1mG-Pred:\t\u001b[0m you are being silly .\n","\u001b[1mW-Pred:\t\u001b[0m you are rude .\n","\u001b[1mF-Pred:\t\u001b[0m you are silly .\n","\u001b[1mS-Pred:\t\u001b[0m you are being silly . \n","\n","G-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.756\n","W-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.830\n","F-Scores:\tBleu: 0.667\tGleu: 0.500\tCustom Score: 0.931\n","S-Scores:\tBleu: 0.500\tGleu: 0.300\tCustom Score: 0.756\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous partons d ici .\n","\u001b[1mTarget:\t\u001b[0m we are getting out of here . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are leaving here .\n","\u001b[1mW-Pred:\t\u001b[0m we are getting close .\n","\u001b[1mF-Pred:\t\u001b[0m we are trying to do something .\n","\u001b[1mS-Pred:\t\u001b[0m we are leaving here . \n","\n","G-Scores:\tBleu: 0.455\tGleu: 0.222\tCustom Score: 0.531\n","W-Scores:\tBleu: 0.455\tGleu: 0.333\tCustom Score: 0.425\n","F-Scores:\tBleu: 0.333\tGleu: 0.167\tCustom Score: 0.853\n","S-Scores:\tBleu: 0.455\tGleu: 0.222\tCustom Score: 0.531\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne vais pas te le repeter .\n","\u001b[1mTarget:\t\u001b[0m i am not going to tell you again . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not going to go not you .\n","\u001b[1mW-Pred:\t\u001b[0m i am not going to play this game .\n","\u001b[1mF-Pred:\t\u001b[0m i am not going to hurt you .\n","\u001b[1mS-Pred:\t\u001b[0m i am not telling you again . \n","\n","G-Scores:\tBleu: 0.750\tGleu: 0.577\tCustom Score: 0.982\n","W-Scores:\tBleu: 0.625\tGleu: 0.538\tCustom Score: 0.935\n","F-Scores:\tBleu: 0.743\tGleu: 0.577\tCustom Score: 0.702\n","S-Scores:\tBleu: 0.597\tGleu: 0.346\tCustom Score: 0.968\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est frais emoulu de l ecole .\n","\u001b[1mTarget:\t\u001b[0m he is fresh from college . \n","\n","\u001b[1mG-Pred:\t\u001b[0m he is fresh out of college .\n","\u001b[1mW-Pred:\t\u001b[0m he is fresh from college .\n","\u001b[1mF-Pred:\t\u001b[0m he is fresh on his way .\n","\u001b[1mS-Pred:\t\u001b[0m he is fresh out of college . \n","\n","G-Scores:\tBleu: 0.667\tGleu: 0.389\tCustom Score: 0.597\n","W-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","F-Scores:\tBleu: 0.500\tGleu: 0.333\tCustom Score: 0.458\n","S-Scores:\tBleu: 0.667\tGleu: 0.389\tCustom Score: 0.597\n","\n","\n","\u001b[1mInput:\t\u001b[0m il n est pas a la maison .\n","\u001b[1mTarget:\t\u001b[0m he is not at home . \n","\n","\u001b[1mG-Pred:\t\u001b[0m he is not at home .\n","\u001b[1mW-Pred:\t\u001b[0m he is not religious .\n","\u001b[1mF-Pred:\t\u001b[0m he is not at home .\n","\u001b[1mS-Pred:\t\u001b[0m he is not home . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.584\tGleu: 0.429\tCustom Score: 0.545\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.779\tGleu: 0.500\tCustom Score: 0.709\n","\n","\n","\u001b[1mInput:\t\u001b[0m j ai une faim de loup .\n","\u001b[1mTarget:\t\u001b[0m i am as hungry as a bear . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am a very learner .\n","\u001b[1mW-Pred:\t\u001b[0m i am really hungry .\n","\u001b[1mF-Pred:\t\u001b[0m i am a good bit .\n","\u001b[1mS-Pred:\t\u001b[0m i am very hungry . \n","\n","G-Scores:\tBleu: 0.402\tGleu: 0.182\tCustom Score: 0.347\n","W-Scores:\tBleu: 0.354\tGleu: 0.182\tCustom Score: 0.311\n","F-Scores:\tBleu: 0.402\tGleu: 0.182\tCustom Score: 0.347\n","S-Scores:\tBleu: 0.354\tGleu: 0.182\tCustom Score: 0.311\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous sommes vieux amis .\n","\u001b[1mTarget:\t\u001b[0m we are friends from way back . \n","\n","\u001b[1mG-Pred:\t\u001b[0m we are friends now .\n","\u001b[1mW-Pred:\t\u001b[0m we are in love .\n","\u001b[1mF-Pred:\t\u001b[0m we are friends from .\n","\u001b[1mS-Pred:\t\u001b[0m we are old friends . \n","\n","G-Scores:\tBleu: 0.455\tGleu: 0.333\tCustom Score: 0.575\n","W-Scores:\tBleu: 0.303\tGleu: 0.167\tCustom Score: 0.439\n","F-Scores:\tBleu: 0.607\tGleu: 0.556\tCustom Score: 0.594\n","S-Scores:\tBleu: 0.455\tGleu: 0.222\tCustom Score: 0.397\n","\n","\n","\u001b[1mInput:\t\u001b[0m je ne suis pas content .\n","\u001b[1mTarget:\t\u001b[0m i am not happy . \n","\n","\u001b[1mG-Pred:\t\u001b[0m i am not happy .\n","\u001b[1mW-Pred:\t\u001b[0m i am not presentable .\n","\u001b[1mF-Pred:\t\u001b[0m i am not happy .\n","\u001b[1mS-Pred:\t\u001b[0m i am dissatisfied . \n","\n","G-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","W-Scores:\tBleu: 0.750\tGleu: 0.600\tCustom Score: 0.713\n","F-Scores:\tBleu: 1.000\tGleu: 1.000\tCustom Score: 1.000\n","S-Scores:\tBleu: 0.478\tGleu: 0.300\tCustom Score: 0.608\n","\n","\n","\u001b[1m\u001b[34mAvg G-Bleu Score  :\u001b[0m 0.705\n","\u001b[1m\u001b[34mAvg G-Gleu Score  :\u001b[0m 0.564\n","\u001b[1m\u001b[34mAvg G-Custom Score:\u001b[0m 0.794\n","\n","\n","\u001b[1m\u001b[34mAvg W-Bleu Score  :\u001b[0m 0.610\n","\u001b[1m\u001b[34mAvg W-Gleu Score  :\u001b[0m 0.481\n","\u001b[1m\u001b[34mAvg W-Custom Score:\u001b[0m 0.655\n","\n","\n","\u001b[1m\u001b[34mAvg F-Bleu Score  :\u001b[0m 0.672\n","\u001b[1m\u001b[34mAvg F-Gleu Score  :\u001b[0m 0.545\n","\u001b[1m\u001b[34mAvg F-Custom Score:\u001b[0m 0.723\n","\n","\n","\u001b[1m\u001b[34mAvg S-Bleu Score  :\u001b[0m 0.672\n","\u001b[1m\u001b[34mAvg S-Gleu Score  :\u001b[0m 0.465\n","\u001b[1m\u001b[34mAvg S-Custom Score:\u001b[0m 0.734\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e5LxuZ-sIeLH"},"source":["Evaluate on entire dataset"]},{"cell_type":"code","metadata":{"id":"X4Jnp__f4Kvq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606780403467,"user_tz":300,"elapsed":50444,"user":{"displayName":"Travis Twigg","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhZo1fSnuBhJ4Nhp2eV7YbBd38SXguQVkaE2XU8Nw=s64","userId":"16089971521895945039"}},"outputId":"ec995e46-3f7d-47b1-b582-9a63bb39d589"},"source":["evaluateAll(encoder1, decoder1, all = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1mEvaluation of Machine Translation Models\u001b[0m\n","\u001b[1mEvaluating entire dataset...\u001b[0m\n","\n","\n","\u001b[1m\u001b[34mAvg Bleu Score  :\u001b[0m 0.984\n","\u001b[1m\u001b[34mAvg Gleu Score  :\u001b[0m 0.973\n","\u001b[1m\u001b[34mAvg Custom Score:\u001b[0m 0.988\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t_iAa6E4KVqA"},"source":["Evaluate Google Translate on random sample of dataset\n"]},{"cell_type":"code","metadata":{"id":"CTWXqwubKYrG"},"source":["# Evaluates Google Translate Service on Dataset for n examples\n","# GT allows a limited number of translations per day - so can't run too many times or on entire dataset\n","# Input: n\n","# Output: none\n","\n","def eval_google_translate_randomly(n=20):\n","    print(bold+'Evaluation of Google Translate\\n')\n","    print('Evaluating',n,'examples...'+reset)\n","\n","    bleu_tracker = []\n","    gleu_tracker = []\n","    custom_tracker = []\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        ref = pair[1].split()\n","        pred = google_translate(pair[0])\n","        pred = normalizeString(pred)\n","        ref, pred = fix_punctuation(ref,pred.split())\n","        ref, pred = fix_contractions(ref,pred)\n"," \n","        print('\\n\\n')\n","        print(bold+'Input:\\t'+reset, pair[0])\n","        print(bold+'Target:\\t'+reset, ' '.join(ref))\n","        print(bold+'Pred:\\t'+reset, ' '.join(pred),'\\n')\n","        \n","        # requires ref to be a 2d list, pred 1d list\n","        bleu_one_gram = bleu([ref[:-1]],pred[:-1])\n","        bleu_tracker.append(bleu_one_gram)\n","        print(f'Bleu Score: {bleu_one_gram:.3f}')\n","\n","        gleu_one_gram = gleu([ref[:-1]],pred[:-1])\n","        gleu_tracker.append(gleu_one_gram)\n","        print(f'Gleu Score: {gleu_one_gram:.3f}')\n","        print(f'Avg Score:  {(gleu_one_gram*.25+bleu_one_gram*.75):.3f}') #weighted\n","        \n","        cs_score = 0\n","\n","        if bleu_one_gram < 1:\n","          try: # sometimes sims returns none\n","            sim_returns = similarities(ref,pred)\n","            cs_score = sim_returns[0]\n","            double_word_penalty = sim_returns[1]\n","            cust_score = custom_score(bleu_one_gram,gleu_one_gram,cs_score,double_word_penalty)\n","            print(bold_red_font_tag+'Custom Score: ',cust_score,reset)\n","            #print('\\n')\n","\n","          except KeyError:\n","            print('Cosine similarities: Word not found in embedding vocabulary')\n","            continue\n","        else:\n","          cust_score = custom_score(bleu_one_gram,gleu_one_gram,0,0)\n","          print(bold_red_font_tag+'Custom Score: ',cust_score,reset)\n","\n","        custom_tracker.append(cust_score)\n","\n","    print('\\n')\n","    print(f'{bold_blue_font_tag}Avg Bleu Score  :{reset} {sum(bleu_tracker)/len(bleu_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg Gleu Score  :{reset} {sum(gleu_tracker)/len(gleu_tracker):.3f}')\n","    print(f'{bold_blue_font_tag}Avg Custom Score:{reset} {sum(custom_tracker)/len(custom_tracker):.3f}')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbfr-hj6L2Jr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ba957ad-6765-4ded-a702-580f152e000b"},"source":["eval_google_translate_randomly()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1mEvaluation of Google Translate\n","\n","Evaluating 20 examples...\u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis introverti .\n","\u001b[1mTarget:\t\u001b[0m i am introverted .\n","\u001b[1mPred:\t\u001b[0m i am an introvert . \n","\n","Bleu Score: 0.500\n","Gleu Score: 0.300\n","Avg Score:  0.450\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['introverted', 'introvert', 0.7447564] \u001b[0m\n","\n","Semantic similarity bonus : + 0.29790256023406986\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.7479025602340699 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m c est un homme riche .\n","\u001b[1mTarget:\t\u001b[0m he is a man of wealth .\n","\u001b[1mPred:\t\u001b[0m he is a rich man . \n","\n","Bleu Score: 0.655\n","Gleu Score: 0.389\n","Avg Score:  0.588\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['wealth', 'rich', 0.45845908] \u001b[0m\n","\n","Semantic similarity bonus : + 0.1833836317062378\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.7718443057752492 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes tres elegante .\n","\u001b[1mTarget:\t\u001b[0m you are very sophisticated .\n","\u001b[1mPred:\t\u001b[0m you are very elegant . \n","\n","Bleu Score: 0.750\n","Gleu Score: 0.600\n","Avg Score:  0.713\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['sophisticated', 'elegant', 0.4337048] \u001b[0m\n","\n","Semantic similarity bonus : + 0.17348191738128663\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.8859819173812866 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes fort en colere .\n","\u001b[1mTarget:\t\u001b[0m you are very angry .\n","\u001b[1mPred:\t\u001b[0m you are very angry . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m il fait du velo .\n","\u001b[1mTarget:\t\u001b[0m he is riding a bicycle .\n","\u001b[1mPred:\t\u001b[0m he is a biker . \n","\n","Bleu Score: 0.584\n","Gleu Score: 0.286\n","Avg Score:  0.510\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['bicycle', 'biker', 0.4419978] \u001b[0m\n","\u001b[1m   ['riding', 'biker', 0.37392926] \u001b[0m\n","\n","Semantic similarity bonus : + 0.32637082338333134\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.835874835289568 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle est toujours occupee les jours de semaine .\n","\u001b[1mTarget:\t\u001b[0m she is always busy on weekdays .\n","\u001b[1mPred:\t\u001b[0m it is always busy on weekdays . \n","\n","Bleu Score: 0.833\n","Gleu Score: 0.778\n","Avg Score:  0.819\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['she', 'it', 0.43182984] \u001b[0m\n","\n","Semantic similarity bonus : + 0.1727319359779358\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.9921763804223802 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est plein d ambition .\n","\u001b[1mTarget:\t\u001b[0m he is full of ambition .\n","\u001b[1mPred:\t\u001b[0m he is full of ambition . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est ecrivain .\n","\u001b[1mTarget:\t\u001b[0m he is a writer .\n","\u001b[1mPred:\t\u001b[0m he is a writer . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m t es une bonne fille .\n","\u001b[1mTarget:\t\u001b[0m you are a good person .\n","\u001b[1mPred:\t\u001b[0m you are a good person . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous mentez n est ce pas ?\n","\u001b[1mTarget:\t\u001b[0m you are lying are not you ?\n","\u001b[1mPred:\t\u001b[0m you are lying are not you ? \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est assis sur la chaise .\n","\u001b[1mTarget:\t\u001b[0m he is sitting on the chair .\n","\u001b[1mPred:\t\u001b[0m he sat on the chair . \n","\n","Bleu Score: 0.655\n","Gleu Score: 0.389\n","Avg Score:  0.588\n","\n","Semantic similarities using w2v:\n","\u001b[1m   ['sitting', 'sat', 0.7401665] \u001b[0m\n","   ['is', 'sat', 0.11343085]\n","\n","Semantic similarity bonus : + 0.29606659412384034\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.8845272681928518 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis heureux que tu sois la .\n","\u001b[1mTarget:\t\u001b[0m i am happy you are here .\n","\u001b[1mPred:\t\u001b[0m i am happy you came . \n","\n","Bleu Score: 0.655\n","Gleu Score: 0.556\n","Avg Score:  0.630\n","\n","Semantic similarities using w2v:\n","   ['here', 'came', 0.2541636]\n","   ['are', 'came', 0.19414374]\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.630127340735678 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes un drole de zozo .\n","\u001b[1mTarget:\t\u001b[0m you are a funny guy .\n","\u001b[1mPred:\t\u001b[0m you are a funny zozo . \n","\n","Bleu Score: 0.800\n","Gleu Score: 0.714\n","Avg Score:  0.779\n","Cosine similarities: Word not found in embedding vocabulary\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m je suis parfaitement heureux .\n","\u001b[1mTarget:\t\u001b[0m i am perfectly happy .\n","\u001b[1mPred:\t\u001b[0m i am perfectly happy . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m je vais a paris a l automne .\n","\u001b[1mTarget:\t\u001b[0m i am going to paris in the fall .\n","\u001b[1mPred:\t\u001b[0m i am going to betting in the fall . \n","\n","Bleu Score: 0.875\n","Gleu Score: 0.615\n","Avg Score:  0.810\n","\n","Semantic similarities using w2v:\n","   ['paris', 'betting', 0.0038579693]\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.8100961538461539 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m nous avons fini de parler .\n","\u001b[1mTarget:\t\u001b[0m we are done talking .\n","\u001b[1mPred:\t\u001b[0m we are done talking . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m je vais dormir .\n","\u001b[1mTarget:\t\u001b[0m i am going to sleep .\n","\u001b[1mPred:\t\u001b[0m i am going to sleep . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m il est vraiment en bonne forme .\n","\u001b[1mTarget:\t\u001b[0m he is really in good shape .\n","\u001b[1mPred:\t\u001b[0m he is really in good shape . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m elle n est pas poete mais romanciere .\n","\u001b[1mTarget:\t\u001b[0m she is not a poet but a novelist .\n","\u001b[1mPred:\t\u001b[0m she is not a poet but a novelist . \n","\n","Bleu Score: 1.000\n","Gleu Score: 1.000\n","Avg Score:  1.000\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  1.0 \u001b[0m\n","\n","\n","\n","\u001b[1mInput:\t\u001b[0m vous etes fort occupe .\n","\u001b[1mTarget:\t\u001b[0m you are very busy .\n","\u001b[1mPred:\t\u001b[0m you are busy . \n","\n","Bleu Score: 0.717\n","Gleu Score: 0.400\n","Avg Score:  0.637\n","\n","Semantic similarity bonus : + 0.0\n","Double word penalty:        - 0.0 \n","\n","\u001b[1m\u001b[31mCustom Score:  0.637398482930342 \u001b[0m\n","\n","\n","\u001b[1m\u001b[34mAvg Bleu Score  :\u001b[0m 0.851\n","\u001b[1m\u001b[34mAvg Gleu Score  :\u001b[0m 0.751\n","\u001b[1m\u001b[34mAvg Custom Score:\u001b[0m 0.905\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uW7RGSDl5XxA"},"source":["Try a custom sentence  \n","It should start with prefixes and be less than 10 words"]},{"cell_type":"code","metadata":{"id":"YHGl09YJtCbF"},"source":["sent = ['i am happy to see you','je suis content de te voir']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYrbYMBRtCeD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cd25435-62d3-47c5-82be-9f5edc1675ac"},"source":["print('Input:\\t\\t', sent[1])\n","print('Target:\\t\\t', sent[0])\n","output_words = evaluate(encoder1, decoder1, sent[1])\n","output_sentence = ' '.join(output_words)\n","print('Prediction:\\t', output_sentence,'\\n')\n","ref = [sent[0].split()]\n","pred = output_sentence.split()[:-2]\n","\n","bleu_one_gram = bleu(ref,pred)\n","gleu_one_gram = gleu(ref,pred)\n","print(f'Bleu Score:\\t {bleu_one_gram:.2f}')\n","print(f'Gleu Score:\\t {gleu_one_gram:.2f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input:\t\t je suis content de te voir\n","Target:\t\t i am happy to see you\n","Prediction:\t i m glad to meet you . <EOS> \n","\n","Bleu Score:\t 0.50\n","Gleu Score:\t 0.17\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CYui5G3LSX9e"},"source":["# References  \n","\n","Robertson, S. (2020). NLP From Scratch: Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 1.7.0 documentation. Https://Pytorch.Org. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"e-cmaqW6Tn2g"},"source":[""],"execution_count":null,"outputs":[]}]}